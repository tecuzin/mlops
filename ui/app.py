from __future__ import annotations

import os
import time

import httpx
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st

API_URL = os.getenv("API_URL", "http://api:8000")

st.set_page_config(page_title="MLOps â€” EntraÃ®nement, Ã‰valuation & SÃ©curitÃ© LLM", layout="wide")
st.title("MLOps â€” EntraÃ®nement, Ã‰valuation & SÃ©curitÃ© LLM")


def api_get(path: str, **params):
    with httpx.Client(base_url=API_URL, timeout=30) as client:
        resp = client.get(path, params=params)
        resp.raise_for_status()
        return resp.json()


def api_post(path: str, **kwargs):
    with httpx.Client(base_url=API_URL, timeout=30) as client:
        resp = client.post(path, **kwargs)
        resp.raise_for_status()
        return resp.json()


OWASP_LABELS = {
    "sec_prompt_injection": "LLM01 â€” Injection d'invites",
    "sec_output_handling": "LLM02 â€” Sorties non sÃ©curisÃ©es",
    "sec_data_poisoning": "LLM03 â€” Empoisonnement des donnÃ©es",
    "sec_model_dos": "LLM04 â€” DÃ©ni de service",
    "sec_supply_chain": "LLM05 â€” ChaÃ®ne logistique",
    "sec_info_disclosure": "LLM06 â€” Divulgation d'informations",
    "sec_overreliance": "LLM09 â€” DÃ©pendance excessive",
    "sec_model_theft": "LLM10 â€” Vol de modÃ¨le",
    "ml_sec_score": "MLSecScore global",
}

STATUS_ICONS = {
    "pending": "â³",
    "training": "ðŸ‹ï¸",
    "evaluating": "ðŸ“Š",
    "security_scanning": "ðŸ”’",
    "completed": "âœ…",
    "failed": "âŒ",
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
tab_config, tab_status, tab_results, tab_security = st.tabs([
    "âš™ï¸ Configuration",
    "ðŸ“¡ Status",
    "ðŸ“Š RÃ©sultats",
    "ðŸ”’ SÃ©curitÃ©",
])


# â”€â”€ Onglet Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_config:
    st.header("Nouvelle expÃ©rience")

    try:
        train_datasets = api_get("/datasets", dataset_type="train")
        eval_datasets = api_get("/datasets", dataset_type="eval")
    except Exception as e:
        st.error(f"Impossible de contacter l'API : {e}")
        train_datasets, eval_datasets = [], []

    if train_datasets or eval_datasets:
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ModÃ¨le")
            experiment_name = st.text_input("Nom de l'expÃ©rience", value="mlops-default")
            model_name = st.text_input("Nom du run", placeholder="mistral-7b-rag-qa")
            model_id = st.text_input(
                "HuggingFace Model ID",
                placeholder="mistralai/Mistral-7B-v0.1",
            )
            task_type = st.selectbox("Type de tÃ¢che", ["finetune", "eval_only", "security_eval"])
            register_model = st.checkbox("Enregistrer dans le Model Registry")

        with col2:
            st.subheader("Datasets")
            if task_type == "finetune":
                train_ds_options = {d["name"]: d["id"] for d in train_datasets}
                selected_train = st.selectbox(
                    "Dataset d'entraÃ®nement",
                    options=list(train_ds_options.keys()),
                )
                train_dataset_id = train_ds_options.get(selected_train)
            elif task_type == "security_eval":
                train_ds_options = {d["name"]: d["id"] for d in train_datasets}
                selected_train = st.selectbox(
                    "Dataset d'entraÃ®nement (optionnel, pour audit PII)",
                    options=["(aucun)"] + list(train_ds_options.keys()),
                )
                train_dataset_id = train_ds_options.get(selected_train)
            else:
                train_dataset_id = None

            if task_type == "security_eval":
                eval_dataset_id = None
                st.info("Pas de dataset d'Ã©valuation requis pour l'analyse de sÃ©curitÃ©.")
            else:
                eval_ds_options = {d["name"]: d["id"] for d in eval_datasets}
                selected_eval = st.selectbox(
                    "Dataset d'Ã©valuation",
                    options=list(eval_ds_options.keys()),
                )
                eval_dataset_id = eval_ds_options.get(selected_eval)

        st.divider()

        col_hp, col_ragas = st.columns(2)

        with col_hp:
            st.subheader("HyperparamÃ¨tres")
            if task_type == "finetune":
                epochs = st.slider("Epochs", 1, 20, 3)
                batch_size = st.select_slider("Batch size", [1, 2, 4, 8, 16], value=4)
                learning_rate = st.number_input("Learning rate", value=2e-5, format="%.1e", step=1e-5)
                warmup_steps = st.number_input("Warmup steps", value=100, step=10)
                max_seq_length = st.select_slider("Max seq length", [128, 256, 512, 768, 1024, 2048], value=512)
                grad_accum = st.select_slider("Gradient accumulation steps", [1, 2, 4, 8, 16], value=4)
                fp16 = st.checkbox("FP16 (mixed precision)", value=True)

                with st.expander("Configuration LoRA (avancÃ©)"):
                    use_lora = st.checkbox("Activer LoRA", value=True)
                    if use_lora:
                        lora_r = st.select_slider("LoRA rank (r)", [4, 8, 16, 32, 64], value=16)
                        lora_alpha = st.select_slider("LoRA alpha", [8, 16, 32, 64, 128], value=32)
                        lora_dropout = st.slider("LoRA dropout", 0.0, 0.5, 0.05, 0.01)
                        lora_modules = st.multiselect(
                            "Target modules",
                            ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                            default=["q_proj", "v_proj"],
                        )
            elif task_type == "security_eval":
                st.info("Configuration de l'analyse de sÃ©curitÃ© ci-contre.")
            else:
                st.info("Pas d'hyperparamÃ¨tres pour l'Ã©valuation seule.")

        with col_ragas:
            if task_type == "security_eval":
                st.subheader("Configuration sÃ©curitÃ© (OWASP Top 10)")
                sec_modelscan = st.checkbox("ModelScan â€” analyse statique des artefacts", value=True)
                sec_data_audit = st.checkbox("Audit des donnÃ©es d'entraÃ®nement (PII)", value=True)
                sec_prompt_injection = st.checkbox("Injection d'invites (LLM01)", value=True)
                sec_pii_leakage = st.checkbox("Divulgation d'informations (LLM06)", value=True)
                sec_toxicity = st.checkbox("ToxicitÃ© / sorties non sÃ©curisÃ©es (LLM02)", value=True)
                sec_bias = st.checkbox("Biais (discrimination)", value=True)
                sec_hallucination = st.checkbox("Hallucinations / sur-confiance (LLM09)", value=True)
                sec_dos = st.checkbox("RÃ©silience DoS (LLM04)", value=True)
                with st.expander("ParamÃ¨tres avancÃ©s"):
                    sec_max_probes = st.number_input("Probes max par catÃ©gorie", value=50, min_value=5, max_value=500, step=5)
                    sec_timeout = st.number_input("Timeout par probe (secondes)", value=300, min_value=30, max_value=3600, step=30)
            else:
                st.subheader("MÃ©triques RAGAS")
                m_faithfulness = st.checkbox("Faithfulness", value=True)
                m_answer_relevancy = st.checkbox("Answer Relevancy", value=True)
                m_context_precision = st.checkbox("Context Precision", value=True)
                m_context_recall = st.checkbox("Context Recall", value=True)

        st.divider()

        if st.button("Valider & lancer le pipeline", type="primary"):
            if not model_name or not model_id:
                st.error("Le nom du run et le Model ID sont obligatoires.")
            elif task_type != "security_eval" and not eval_dataset_id:
                st.error("Un dataset d'Ã©valuation est obligatoire.")
            else:
                payload: dict = {
                    "experiment_name": experiment_name,
                    "model_name": model_name,
                    "model_id": model_id,
                    "task_type": task_type,
                    "train_dataset_id": train_dataset_id,
                    "eval_dataset_id": eval_dataset_id,
                    "register_model": register_model,
                }

                if task_type == "security_eval":
                    payload["security_config"] = {
                        "modelscan_enabled": sec_modelscan,
                        "training_data_audit": sec_data_audit,
                        "prompt_injection": sec_prompt_injection,
                        "pii_leakage": sec_pii_leakage,
                        "toxicity": sec_toxicity,
                        "bias": sec_bias,
                        "hallucination": sec_hallucination,
                        "dos_resilience": sec_dos,
                        "max_probes_per_category": sec_max_probes,
                        "timeout_per_probe_seconds": sec_timeout,
                    }
                else:
                    payload["ragas_metrics"] = {
                        "faithfulness": m_faithfulness,
                        "answer_relevancy": m_answer_relevancy,
                        "context_precision": m_context_precision,
                        "context_recall": m_context_recall,
                    }

                if task_type == "finetune":
                    lora_cfg = None
                    if use_lora:
                        lora_cfg = {
                            "r": lora_r,
                            "lora_alpha": lora_alpha,
                            "lora_dropout": lora_dropout,
                            "target_modules": lora_modules,
                        }
                    payload["training_params"] = {
                        "epochs": epochs,
                        "batch_size": batch_size,
                        "learning_rate": learning_rate,
                        "warmup_steps": warmup_steps,
                        "max_seq_length": max_seq_length,
                        "gradient_accumulation_steps": grad_accum,
                        "fp16": fp16,
                        "lora": lora_cfg,
                    }

                try:
                    result = api_post("/runs", json=payload)
                    st.success(f"Pipeline lancÃ© ! Run ID : **{result['id']}**")
                except Exception as e:
                    st.error(f"Erreur lors du lancement : {e}")


# â”€â”€ Onglet Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_status:
    st.header("Suivi des pipelines")

    if st.button("RafraÃ®chir", key="refresh_status"):
        st.rerun()

    try:
        all_runs = api_get("/runs")
    except Exception as e:
        st.error(f"Impossible de contacter l'API : {e}")
        all_runs = []

    if not all_runs:
        st.info("Aucun run lancÃ© pour le moment.")
    else:
        for run in all_runs:
            icon = STATUS_ICONS.get(run["status"], "â“")
            with st.expander(
                f"{icon} **{run['model_name']}** â€” {run['status'].upper()} â€” "
                f"ID {run['id']} â€” {run['created_at'][:19]}",
                expanded=run["status"] not in ("completed", "failed"),
            ):
                col_relaunch, col_spacer = st.columns([1, 4])
                with col_relaunch:
                    relaunch_clicked = st.button(
                        "ðŸ”„ Relancer",
                        key=f"relaunch_{run['id']}",
                        type="primary",
                    )
                if relaunch_clicked:
                    snapshot = run.get("config_snapshot", {})
                    payload = {
                        "experiment_name": snapshot.get("experiment_name", run["experiment_name"]),
                        "model_name": snapshot.get("model_name", run["model_name"]),
                        "model_id": snapshot.get("model_id", run["model_id"]),
                        "task_type": snapshot.get("task_type", run["task_type"]),
                        "train_dataset_id": snapshot.get("train_dataset_id"),
                        "eval_dataset_id": snapshot.get("eval_dataset_id"),
                        "register_model": snapshot.get("register_model", False),
                    }
                    if snapshot.get("ragas_metrics"):
                        payload["ragas_metrics"] = snapshot["ragas_metrics"]
                    if snapshot.get("security_config"):
                        payload["security_config"] = snapshot["security_config"]
                    if snapshot.get("training_params"):
                        payload["training_params"] = snapshot["training_params"]
                    try:
                        new_run = api_post("/runs", json=payload)
                        st.success(f"Run relancÃ© ! Nouveau Run ID : **{new_run['id']}**")
                        time.sleep(1)
                        st.rerun()
                    except Exception as e:
                        st.error(f"Erreur lors de la relance : {e}")

                col_info, col_progress = st.columns([1, 2])

                with col_info:
                    st.markdown(f"**ModÃ¨le :** `{run['model_id']}`")
                    st.markdown(f"**TÃ¢che :** {run['task_type']}")
                    st.markdown(f"**ExpÃ©rience :** {run['experiment_name']}")
                    if run.get("mlflow_run_id"):
                        st.markdown(f"**MLflow Run :** `{run['mlflow_run_id']}`")
                    if run.get("error_message"):
                        st.error(run["error_message"])

                with col_progress:
                    status = run["status"]
                    if status == "pending":
                        st.progress(0.0, text="En attente...")
                    elif status == "training":
                        st.progress(0.33, text="EntraÃ®nement en cours...")
                    elif status == "evaluating":
                        st.progress(0.66, text="Ã‰valuation en cours...")
                    elif status == "security_scanning":
                        st.progress(0.50, text="Analyse de sÃ©curitÃ© en cours...")
                    elif status == "completed":
                        st.progress(1.0, text="TerminÃ©")
                    elif status == "failed":
                        st.progress(1.0, text="Ã‰chouÃ©")

                    if run.get("results"):
                        st.markdown("**RÃ©sultats provisoires :**")
                        for r in run["results"]:
                            label = r["metric_name"]
                            if label.startswith("sec_"):
                                label = OWASP_LABELS.get(r["metric_name"], label.replace("sec_", "").replace("_", " ").title())
                            st.metric(label, f"{r['metric_value']:.4f}")

                if run.get("logs"):
                    with st.expander("Logs dÃ©taillÃ©s"):
                        st.code(run["logs"], language="text")


# â”€â”€ Onglet RÃ©sultats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_results:
    st.header("RÃ©sultats RAGAS & Comparaison")

    if st.button("RafraÃ®chir", key="refresh_results"):
        st.rerun()

    try:
        all_completed = [r for r in api_get("/runs") if r["status"] == "completed"]
    except Exception as e:
        st.error(f"Impossible de contacter l'API : {e}")
        all_completed = []

    ragas_runs = [r for r in all_completed if r["task_type"] != "security_eval"]

    if not ragas_runs:
        st.info("Aucun run d'entraÃ®nement / Ã©valuation terminÃ© pour le moment.")
    else:
        ragas_metric_names = ["faithfulness", "answer_relevancy", "context_precision", "context_recall", "ml_score"]
        training_metric_names = ["train_loss", "perplexity", "train_runtime", "train_samples_per_second"]

        try:
            all_datasets = {d["id"]: d["name"] for d in api_get("/datasets")}
        except Exception:
            all_datasets = {}

        def _lifecycle_tag(run):
            if run.get("mlflow_model_version"):
                return "finetuned"
            if run["task_type"] == "finetune":
                return "trained"
            return ""

        def _domain_tag(run):
            for key in ("train_dataset_id", "eval_dataset_id"):
                ds_id = run.get(key) or run.get("config_snapshot", {}).get(key)
                if ds_id:
                    name = all_datasets.get(ds_id, "").lower()
                    if "medical" in name:
                        return "medic"
                    if "legal" in name:
                        return "legal"
            return ""

        def _validation_tag(run):
            for r in run.get("results", []):
                if r["metric_name"] == "ml_score":
                    return "validated" if r["metric_value"] >= 0.7 else "rejected"
            return ""
        # â”€â”€ Tableau des mÃ©triques RAGAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.subheader("Scores RAGAS")
        rows = []
        for run in ragas_runs:
            metrics = {r["metric_name"]: r["metric_value"] for r in run.get("results", [])}
            row = {
                "Run ID": run["id"],
                "ModÃ¨le": run["model_name"],
                "Model ID": run["model_id"],
                "TÃ¢che": run["task_type"],
                "Lifecycle": _lifecycle_tag(run),
                "Domaine": _domain_tag(run),
                "Validation": _validation_tag(run),
                "Date": run["created_at"][:19],
            }
            for m in ragas_metric_names:
                if m in metrics:
                    row[m] = round(metrics[m], 4)
            rows.append(row)

        df = pd.DataFrame(rows)
        st.dataframe(df, hide_index=True)

        NON_METRIC_COLS = {"Run ID", "ModÃ¨le", "Model ID", "TÃ¢che", "Lifecycle", "Domaine", "Validation", "Date"}
        metric_cols = [c for c in df.columns if c not in NON_METRIC_COLS]

        if metric_cols:
            st.subheader("Comparaison graphique")
            df_melted = df.melt(
                id_vars=["ModÃ¨le"],
                value_vars=metric_cols,
                var_name="MÃ©trique",
                value_name="Score",
            )
            fig = px.bar(
                df_melted,
                x="MÃ©trique",
                y="Score",
                color="ModÃ¨le",
                barmode="group",
                title="Scores RAGAS par modÃ¨le",
                range_y=[0, 1],
            )
            fig.update_layout(height=450)
            st.plotly_chart(fig)

            st.subheader("Radar des scores RAGAS")
            fig_radar = go.Figure()
            for _, row_data in df.iterrows():
                values = [row_data.get(m, 0) for m in metric_cols]
                values.append(values[0])
                fig_radar.add_trace(go.Scatterpolar(
                    r=values,
                    theta=metric_cols + [metric_cols[0]],
                    name=row_data["ModÃ¨le"],
                    fill="toself",
                    opacity=0.6,
                ))
            fig_radar.update_layout(
                polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
                title="Radar des mÃ©triques RAGAS",
                height=450,
            )
            st.plotly_chart(fig_radar)

        if len(ragas_runs) > 1 and metric_cols:
            st.subheader("ModÃ¨le champion")
            df_scores = df[metric_cols].copy()
            df["score_moyen"] = df_scores.mean(axis=1)
            champion_idx = df["score_moyen"].idxmax()
            champion = df.loc[champion_idx]
            st.success(
                f"Le meilleur modÃ¨le est **{champion['ModÃ¨le']}** "
                f"avec un score moyen de **{champion['score_moyen']:.4f}**"
            )

        # â”€â”€ Tableau des mÃ©triques d'entraÃ®nement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.divider()
        st.subheader("MÃ©triques d'entraÃ®nement")
        train_rows = []
        for run in ragas_runs:
            metrics = {r["metric_name"]: r["metric_value"] for r in run.get("results", [])}
            row = {"ModÃ¨le": run["model_name"]}
            for m in training_metric_names:
                if m in metrics:
                    row[m] = round(metrics[m], 4)
            train_rows.append(row)

        df_train = pd.DataFrame(train_rows)
        st.dataframe(df_train, hide_index=True)

        # â”€â”€ Export CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.divider()
        st.subheader("Export")
        all_export_rows = []
        for run in ragas_runs:
            row = {
                "Run ID": run["id"],
                "ModÃ¨le": run["model_name"],
                "Model ID": run["model_id"],
                "TÃ¢che": run["task_type"],
                "Date": run["created_at"][:19],
            }
            for r in run.get("results", []):
                row[r["metric_name"]] = r["metric_value"]
            all_export_rows.append(row)

        df_export = pd.DataFrame(all_export_rows)
        csv_data = df_export.to_csv(index=False).encode("utf-8")
        st.download_button(
            label="TÃ©lÃ©charger les rÃ©sultats RAGAS (.csv)",
            data=csv_data,
            file_name="resultats_ragas.csv",
            mime="text/csv",
        )


# â”€â”€ Onglet SÃ©curitÃ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_security:
    st.header("Ã‰valuations de sÃ©curitÃ© â€” OWASP Top 10 LLM")

    if st.button("RafraÃ®chir", key="refresh_security"):
        st.rerun()

    try:
        all_runs_sec = api_get("/runs")
    except Exception as e:
        st.error(f"Impossible de contacter l'API : {e}")
        all_runs_sec = []

    security_runs = [r for r in all_runs_sec if r["task_type"] == "security_eval"]
    completed_sec = [r for r in security_runs if r["status"] == "completed"]
    active_sec = [r for r in security_runs if r["status"] not in ("completed", "failed")]
    failed_sec = [r for r in security_runs if r["status"] == "failed"]

    if not security_runs:
        st.info(
            "Aucune Ã©valuation de sÃ©curitÃ© lancÃ©e pour le moment.\n\n"
            "Rendez-vous dans l'onglet **Configuration** pour crÃ©er un run de type `security_eval`."
        )
    else:
        # â”€â”€ Active runs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if active_sec:
            st.subheader("Ã‰valuations en cours")
            for run in active_sec:
                icon = STATUS_ICONS.get(run["status"], "ðŸ”’")
                st.info(f"{icon} **{run['model_name']}** â€” {run['status'].upper()} (ID {run['id']})")

        # â”€â”€ Failed runs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if failed_sec:
            st.subheader("Ã‰valuations Ã©chouÃ©es")
            for run in failed_sec:
                with st.expander(f"âŒ {run['model_name']} (ID {run['id']})"):
                    if run.get("error_message"):
                        st.error(run["error_message"])
                    if run.get("logs"):
                        st.code(run["logs"][-2000:], language="text")

        # â”€â”€ Completed results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if completed_sec:
            st.subheader("RÃ©sultats de sÃ©curitÃ©")

            owasp_metrics = [
                "sec_prompt_injection",
                "sec_output_handling",
                "sec_data_poisoning",
                "sec_model_dos",
                "sec_supply_chain",
                "sec_info_disclosure",
                "sec_overreliance",
                "sec_model_theft",
            ]

            sec_rows = []
            for run in completed_sec:
                metrics = {r["metric_name"]: r["metric_value"] for r in run.get("results", [])}
                row = {
                    "Run ID": run["id"],
                    "ModÃ¨le": run["model_name"],
                    "Date": run["created_at"][:19],
                }
                for m in owasp_metrics:
                    label = OWASP_LABELS.get(m, m)
                    row[label] = round(metrics.get(m, 0), 4)
                row["MLSecScore"] = round(metrics.get("ml_sec_score", 0), 4)
                sec_rows.append(row)

            df_sec = pd.DataFrame(sec_rows)
            st.dataframe(df_sec, hide_index=True)

            # â”€â”€ MLSecScore badges â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            st.subheader("MLSecScore")
            badge_cols = st.columns(len(completed_sec))
            for i, run in enumerate(completed_sec):
                mlsecscore = next(
                    (r["metric_value"] for r in run.get("results", []) if r["metric_name"] == "ml_sec_score"),
                    None,
                )
                with badge_cols[i]:
                    if mlsecscore is not None:
                        color = "green" if mlsecscore >= 0.7 else ("orange" if mlsecscore >= 0.4 else "red")
                        st.metric(
                            label=run["model_name"],
                            value=f"{mlsecscore:.4f}",
                        )
                        st.markdown(f":{color}[{'Bon' if mlsecscore >= 0.7 else 'Moyen' if mlsecscore >= 0.4 else 'Faible'}]")

            # â”€â”€ Radar chart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            owasp_radar_labels = [OWASP_LABELS[m] for m in owasp_metrics]

            st.subheader("Radar de sÃ©curitÃ© OWASP Top 10")
            fig_sec = go.Figure()
            for run in completed_sec:
                metrics = {r["metric_name"]: r["metric_value"] for r in run.get("results", [])}
                values = [metrics.get(m, 0) for m in owasp_metrics]
                values.append(values[0])
                fig_sec.add_trace(go.Scatterpolar(
                    r=values,
                    theta=owasp_radar_labels + [owasp_radar_labels[0]],
                    name=run["model_name"],
                    fill="toself",
                    opacity=0.6,
                ))
            fig_sec.update_layout(
                polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
                title="Profil de sÃ©curitÃ© OWASP Top 10",
                height=500,
            )
            st.plotly_chart(fig_sec)

            # â”€â”€ Bar chart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            st.subheader("Comparaison par catÃ©gorie OWASP")
            bar_rows = []
            for run in completed_sec:
                metrics = {r["metric_name"]: r["metric_value"] for r in run.get("results", [])}
                for m in owasp_metrics:
                    bar_rows.append({
                        "ModÃ¨le": run["model_name"],
                        "CatÃ©gorie": OWASP_LABELS.get(m, m),
                        "Score": metrics.get(m, 0),
                    })
            df_bar = pd.DataFrame(bar_rows)
            fig_bar = px.bar(
                df_bar,
                x="CatÃ©gorie",
                y="Score",
                color="ModÃ¨le",
                barmode="group",
                title="Scores de sÃ©curitÃ© par catÃ©gorie OWASP",
                range_y=[0, 1],
            )
            fig_bar.update_layout(height=450, xaxis_tickangle=-30)
            st.plotly_chart(fig_bar)

            # â”€â”€ Detailed logs per run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            st.divider()
            st.subheader("DÃ©tails par Ã©valuation")
            for run in completed_sec:
                with st.expander(f"ðŸ”’ {run['model_name']} â€” ID {run['id']}"):
                    metrics = {r["metric_name"]: r["metric_value"] for r in run.get("results", [])}
                    cols = st.columns(4)
                    for idx, m in enumerate(owasp_metrics):
                        label = OWASP_LABELS.get(m, m)
                        val = metrics.get(m, 0)
                        with cols[idx % 4]:
                            st.metric(label.split(" â€” ")[0], f"{val:.4f}")
                    if run.get("logs"):
                        with st.expander("Logs"):
                            st.code(run["logs"][-3000:], language="text")

            # â”€â”€ Export CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            st.divider()
            st.subheader("Export")
            csv_sec = df_sec.to_csv(index=False).encode("utf-8")
            st.download_button(
                label="TÃ©lÃ©charger les rÃ©sultats sÃ©curitÃ© (.csv)",
                data=csv_sec,
                file_name="resultats_securite.csv",
                mime="text/csv",
            )
