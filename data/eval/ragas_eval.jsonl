{"question": "Qu'est-ce que le RAG en intelligence artificielle ?", "answer": "Le RAG (Retrieval-Augmented Generation) est une technique qui combine la recherche d'information dans une base de connaissances avec la génération de texte par un LLM pour produire des réponses plus précises et factuelles.", "contexts": ["Le RAG est une architecture qui augmente les capacités des modèles de langage en leur fournissant des documents pertinents extraits d'une base de connaissances avant la génération.", "Cette approche réduit les hallucinations en ancrant les réponses dans des sources vérifiables."], "ground_truth": "Le RAG (Retrieval-Augmented Generation) combine la recherche documentaire avec la génération de texte par un LLM pour produire des réponses plus précises et ancrées dans des sources factuelles."}
{"question": "Comment fonctionne le fine-tuning d'un LLM ?", "answer": "Le fine-tuning consiste à réentraîner un modèle pré-entraîné sur un jeu de données spécifique à une tâche, en ajustant ses poids pour améliorer ses performances sur cette tâche particulière.", "contexts": ["Le fine-tuning est une technique de transfer learning où un modèle pré-entraîné est adapté à une tâche spécifique en continuant l'entraînement sur des données ciblées.", "Des techniques comme LoRA permettent un fine-tuning efficace en ne modifiant qu'une fraction des paramètres du modèle."], "ground_truth": "Le fine-tuning adapte un modèle pré-entraîné à une tâche spécifique en poursuivant l'entraînement sur des données spécialisées, ajustant les poids du modèle."}
{"question": "Qu'est-ce que la tokenisation en NLP ?", "answer": "La tokenisation est le processus de découpage d'un texte en unités plus petites appelées tokens, qui peuvent être des mots, des sous-mots ou des caractères, servant d'entrée aux modèles de traitement du langage.", "contexts": ["La tokenisation est la première étape du pipeline NLP. Elle convertit le texte brut en séquences de tokens que le modèle peut traiter.", "Les tokeniseurs modernes comme BPE (Byte Pair Encoding) ou SentencePiece découpent le texte en sous-mots pour gérer le vocabulaire de manière efficace."], "ground_truth": "La tokenisation découpe un texte en unités élémentaires (tokens) — mots, sous-mots ou caractères — qui constituent l'entrée des modèles NLP."}
{"question": "Quels sont les principaux types d'embeddings ?", "answer": "Les principaux types sont les word embeddings (Word2Vec, GloVe), les embeddings contextuels (BERT, GPT) et les sentence embeddings (Sentence-BERT), chacun capturant différents niveaux de sens sémantique.", "contexts": ["Les embeddings sont des représentations vectorielles denses du texte dans un espace continu. Word2Vec et GloVe produisent des vecteurs fixes par mot.", "Les modèles comme BERT génèrent des embeddings contextuels où la représentation d'un mot dépend de son contexte. Sentence-BERT étend cette idée aux phrases entières."], "ground_truth": "Les principaux types d'embeddings incluent les word embeddings statiques (Word2Vec, GloVe), les embeddings contextuels (BERT) et les sentence embeddings (SBERT)."}
{"question": "Comment évaluer la qualité d'un système RAG ?", "answer": "On évalue un système RAG avec des métriques comme la faithfulness (fidélité aux sources), l'answer relevancy (pertinence de la réponse), la context precision et le context recall, souvent mesurées avec le framework RAGAS.", "contexts": ["RAGAS (Retrieval Augmented Generation Assessment) est un framework d'évaluation qui mesure la qualité des systèmes RAG selon plusieurs dimensions.", "La faithfulness mesure si la réponse est fidèle au contexte fourni. L'answer relevancy évalue la pertinence de la réponse par rapport à la question."], "ground_truth": "La qualité d'un RAG s'évalue via des métriques RAGAS : faithfulness, answer relevancy, context precision et context recall."}
{"question": "Qu'est-ce que LoRA et pourquoi l'utiliser ?", "answer": "LoRA (Low-Rank Adaptation) est une technique de fine-tuning efficace qui injecte des matrices de faible rang dans les couches du modèle, permettant d'adapter un LLM avec beaucoup moins de paramètres entraînables et de mémoire.", "contexts": ["LoRA réduit le nombre de paramètres entraînables en décomposant les mises à jour de poids en deux matrices de faible rang.", "Cette technique permet de fine-tuner des modèles de plusieurs milliards de paramètres sur un seul GPU consommateur, car seules les matrices LoRA sont stockées en mémoire."], "ground_truth": "LoRA est une méthode de fine-tuning paramétrique efficient qui utilise des matrices de faible rang pour adapter un LLM avec une fraction des paramètres, réduisant mémoire et coût de calcul."}
{"question": "Comment fonctionne l'attention dans les Transformers ?", "answer": "Le mécanisme d'attention calcule des scores de similarité entre chaque paire de tokens via les matrices Query, Key et Value, permettant au modèle de pondérer l'importance relative de chaque token pour produire une représentation contextuelle.", "contexts": ["L'attention multi-têtes (multi-head attention) est le composant central des Transformers. Chaque tête calcule Q, K, V à partir de l'entrée.", "Le score d'attention est calculé comme softmax(QK^T / sqrt(d_k)) * V, où d_k est la dimension des clés. Cela permet à chaque position de 'regarder' toutes les autres."], "ground_truth": "L'attention dans les Transformers calcule des scores de similarité entre tokens via les matrices Q, K, V et applique softmax pour pondérer les contributions de chaque token."}
{"question": "Qu'est-ce que MLflow et à quoi sert-il ?", "answer": "MLflow est une plateforme open-source pour gérer le cycle de vie du machine learning : tracking d'expériences, packaging de modèles, registre de modèles et déploiement.", "contexts": ["MLflow fournit quatre composants principaux : Tracking pour logger les paramètres et métriques, Projects pour packager le code, Models pour le format de modèle et Registry pour la gestion des versions.", "MLflow s'intègre avec la plupart des frameworks ML (PyTorch, TensorFlow, HuggingFace) et permet de comparer facilement les expériences via son interface web."], "ground_truth": "MLflow est une plateforme open-source de gestion du cycle de vie ML couvrant le tracking d'expériences, le packaging, le registre et le déploiement de modèles."}
